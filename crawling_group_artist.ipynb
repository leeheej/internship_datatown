{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32140a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from urllib.parse import unquote # URL 디코딩 -> ')' 의 경우 %29로 인코딩 되어있어서 href로 가져올 때 디코딩 필요\n",
    "from urllib.parse import quote # URL 인코딩 -> ' '의 경우 %20으로 인코딩 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63114d95",
   "metadata": {},
   "source": [
    "## K-POP Fandom Wiki API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2cf0c8",
   "metadata": {},
   "source": [
    "- Group Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694c066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KpopGroupCrawler_memberDict:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://kpop.fandom.com/api.php\"\n",
    "        self.base_wiki_url = \"https://kpop.fandom.com\"\n",
    "    \n",
    "    def get_group_info(self, group_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        특정 K-pop 그룹의 정보를 크롤링합니다.\n",
    "        \n",
    "        Args:\n",
    "            group_name (str): 그룹명 (예: \"Red_Velvet\", \"BTS\", \"BLACKPINK\")\n",
    "        \n",
    "        Returns:\n",
    "            Dict: 그룹 정보가 담긴 딕셔너리\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"action\": \"parse\",\n",
    "            \"page\": group_name,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            res = requests.get(self.base_url, params=params)\n",
    "            res.raise_for_status()\n",
    "            data = res.json()\n",
    "            \n",
    "            if 'parse' not in data:\n",
    "                print(f\"페이지를 찾을 수 없습니다: {group_name}\")\n",
    "                return {}\n",
    "            \n",
    "            html = data[\"parse\"][\"text\"][\"*\"]\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            group_info = {\n",
    "                'group_name_en': group_name.replace('_', ' '),\n",
    "                'group_name_hangul': self._get_hangul_name(soup),\n",
    "                'debut_date': self._get_debut_date(soup),\n",
    "                'entertainment': self._get_entertainment(soup),\n",
    "                'entertainment_link': self._get_entertainment_link(soup),\n",
    "                'members': self._get_members(soup),\n",
    "                'fandom_name': self._get_fandom_name(soup),\n",
    "                'sns_links': self._get_sns_links(soup)\n",
    "            }\n",
    "            \n",
    "            return group_info\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"네트워크 오류: {e}\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"크롤링 오류: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _get_hangul_name(self, soup) -> str:\n",
    "        \"\"\"한글 그룹명 추출\"\"\"\n",
    "        group_name = soup.select_one('[data-source=\"hangul\"] .pi-data-value')\n",
    "        return group_name.get_text(strip=True) if group_name else \"\"\n",
    "    \n",
    "    def _get_debut_date(self, soup) -> str:\n",
    "        \"\"\"데뷔 날짜 추출\"\"\"\n",
    "        debut_block = soup.select_one('[data-source=\"debut\"] .pi-data-value')\n",
    "        if debut_block:\n",
    "            debut_parts = debut_block.get_text(\"||\", strip=True).split(\"||\")\n",
    "            first_debut = debut_parts[0]\n",
    "            first_debut_date_str = first_debut.split(\"(\")[0].strip()\n",
    "            \n",
    "            try:\n",
    "                debut_date = datetime.strptime(first_debut_date_str, \"%B %d, %Y\")\n",
    "                return debut_date.strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                return first_debut_date_str\n",
    "        return \"\"\n",
    "    \n",
    "    def _get_entertainment(self, soup) -> str:\n",
    "        \"\"\"소속사명 추출 - 첫 번째 엔터테인먼트 회사\"\"\"\n",
    "        label_block = soup.select_one('[data-source=\"label\"] .pi-data-value')\n",
    "        if not label_block:\n",
    "            return \"\"\n",
    "        \n",
    "        # 첫 번째 b 태그(국가 라벨) 다음의 첫 번째 a 태그나 텍스트 찾기\n",
    "        first_b = label_block.find('b')\n",
    "        if first_b:\n",
    "            # b 태그 다음의 첫 번째 a 태그 찾기\n",
    "            a_tag = first_b.find_next('a')\n",
    "            if a_tag:\n",
    "                return a_tag.get_text(strip=True)\n",
    "        \n",
    "        # b 태그가 없으면 기존 방식\n",
    "        labels = label_block.get_text(\"||\", strip=True).split(\"||\")\n",
    "        return labels[0].strip() if labels else \"\"\n",
    "    \n",
    "    def _get_entertainment_link(self, soup) -> str:\n",
    "        \"\"\"소속사 링크 추출 - 첫 번째 엔터테인먼트 회사 링크\"\"\"\n",
    "        label_block = soup.select_one('[data-source=\"label\"] .pi-data-value')\n",
    "        if not label_block:\n",
    "            return \"\"\n",
    "        \n",
    "        # 첫 번째 b 태그(국가 라벨) 다음의 첫 번째 a 태그 찾기\n",
    "        first_b = label_block.find('b')\n",
    "        if first_b:\n",
    "            a_tag = first_b.find_next('a', href=True)\n",
    "            if a_tag:\n",
    "                href = a_tag['href']\n",
    "                return href if href.startswith('http') else f\"{self.base_wiki_url}{href}\"\n",
    "        \n",
    "        # b 태그가 없으면 기존 방식\n",
    "        labels = label_block.get_text(\"||\", strip=True).split(\"||\")\n",
    "        if labels:\n",
    "            first_label = labels[0].strip()\n",
    "            a_tag = label_block.find('a', string=lambda text: text and text.strip() == first_label)\n",
    "            if a_tag and a_tag.has_attr('href'):\n",
    "                href = a_tag['href']\n",
    "                return href if href.startswith('http') else f\"{self.base_wiki_url}{href}\"\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _get_members(self, soup) -> List[Dict]:\n",
    "        \"\"\"멤버 정보 추출\"\"\"\n",
    "        members = []\n",
    "        \n",
    "        # 현재 활동 멤버\n",
    "        members_current = soup.select_one('[data-source=\"current\"] .pi-data-value > ul')\n",
    "        if members_current:\n",
    "            for a in members_current.select('a'):\n",
    "                name = a.get_text(strip=True)\n",
    "                href = a.get(\"href\")\n",
    "                link = f\"{self.base_wiki_url}{href}\"\n",
    "                members.append({\"name\": name, \"href\": link, \"status\": \"current\"})\n",
    "        \n",
    "        # 비활동 멤버\n",
    "        members_inactive = soup.select_one('[data-source=\"inactive\"] .pi-data-value > ul')\n",
    "        if members_inactive:\n",
    "            for a in members_inactive.select('a'):\n",
    "                name = a.get_text(strip=True)\n",
    "                href = a.get(\"href\")\n",
    "                link = f\"{self.base_wiki_url}{href}\"\n",
    "                members.append({\"name\": name, \"href\": link, \"status\": \"inactive\"})\n",
    "        \n",
    "        # 탈퇴 멤버\n",
    "        members_former = soup.select_one('[data-source=\"former\"] .pi-data-value > ul')\n",
    "        if members_former:\n",
    "            for a in members_former.select('a'):\n",
    "                name = a.get_text(strip=True)\n",
    "                href = a.get(\"href\")\n",
    "                link = f\"{self.base_wiki_url}{href}\"\n",
    "                members.append({\"name\": name, \"href\": link, \"status\": \"former\"})\n",
    "        \n",
    "        return members\n",
    "    \n",
    "    def _get_fandom_name(self, soup) -> str:\n",
    "        \"\"\"팬덤명 추출\"\"\"\n",
    "        fandom_block = soup.select_one('[data-source=\"fandom\"] .pi-data-value')\n",
    "        return fandom_block.get_text(strip=True) if fandom_block else \"\"\n",
    "    \n",
    "    def _get_sns_links(self, soup) -> Dict:\n",
    "        \"\"\"SNS 링크 추출\"\"\"\n",
    "        sns_block = soup.select_one('[data-source=\"sns\"]')\n",
    "        sns_data = {}\n",
    "        current_country = \"KR\"\n",
    "        \n",
    "        if sns_block:\n",
    "            for elem in sns_block.children:\n",
    "                if elem.name == 'b':\n",
    "                    country_text = elem.get_text(strip=True).rstrip(':')\n",
    "                    current_country = country_text\n",
    "                    if current_country not in sns_data:\n",
    "                        sns_data[current_country] = []\n",
    "                \n",
    "                if elem.name == 'span':\n",
    "                    a = elem.find('a')\n",
    "                    img = elem.find('img')\n",
    "                    if a and img:\n",
    "                        href = a.get('href')\n",
    "                        platform = img.get('data-image-name', \"\").replace(\" Icon.png\", \"\")\n",
    "                        if current_country not in sns_data:\n",
    "                            sns_data[current_country] = []\n",
    "                        sns_data[current_country].append({\"platform\": platform, \"href\": href})\n",
    "        \n",
    "        return sns_data\n",
    "    \n",
    "    def create_csv_from_groups(self, group_names: List[str], output_filename: str = \"kpop_groups_info.csv\", group_type: str = \"Unknown\"):\n",
    "        \"\"\"\n",
    "        여러 그룹의 정보를 크롤링하여 CSV 파일로 저장하고, 멤버 페이지명 리스트도 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            group_names (List[str]): 크롤링할 그룹명 리스트\n",
    "            output_filename (str): 출력할 CSV 파일명\n",
    "            group_type (str): 그룹 타입 (예: \"Girl Group\", \"Boy Group\", \"Co-ed\", \"Solo\" 등)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (DataFrame, List[Dict]) - CSV DataFrame과 멤버 페이지명 리스트\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        member_pages = []  # API 호출용 멤버 페이지명 리스트\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # 현재 시간\n",
    "        \n",
    "        for group_name in group_names:\n",
    "            print(f\"크롤링 중: {group_name}\")\n",
    "            group_info = self.get_group_info(group_name)\n",
    "            \n",
    "            # 크롤링 실패 시에도 null 값으로 데이터 추가\n",
    "            if not group_info:\n",
    "                print(f\"정보를 가져올 수 없습니다: {group_name}\")\n",
    "                group_info = {\n",
    "                    'group_name_en': group_name.replace('_', ' '),\n",
    "                    'group_name_hangul': None,\n",
    "                    'debut_date': None,\n",
    "                    'entertainment': None,\n",
    "                    'entertainment_link': None,\n",
    "                    'members': [],\n",
    "                    'fandom_name': None,\n",
    "                    'sns_links': {}\n",
    "                }\n",
    "            \n",
    "            # 멤버 페이지명 추출 (current, inactive만 포함, former 제외)\n",
    "            # 중복 없이 추가\n",
    "            for member in group_info['members']:\n",
    "                if member['status'] in ['current', 'inactive']:  # former 제외\n",
    "                    page_name = member['href'].split('/wiki/')[-1] if '/wiki/' in member['href'] else member['name']\n",
    "                    \n",
    "                    # 중복 체크\n",
    "                    if not any(mp['page_name'] == page_name for mp in member_pages):\n",
    "                        member_pages.append({\n",
    "                            'group_name': group_name.replace('_', ' '),\n",
    "                            'page_name': page_name,\n",
    "                            'member_name': member['name']\n",
    "                        })\n",
    "            \n",
    "            # 멤버를 상태별로 분리\n",
    "            current_members = []\n",
    "            inactive_members = []\n",
    "            former_members = []\n",
    "            \n",
    "            for member in group_info['members']:\n",
    "                if member['status'] == 'current':\n",
    "                    current_members.append(member['name'])\n",
    "                elif member['status'] == 'inactive':\n",
    "                    inactive_members.append(member['name'])\n",
    "                elif member['status'] == 'former':\n",
    "                    former_members.append(member['name'])\n",
    "            \n",
    "            # 멤버 정보를 문자열로 변환 (null 처리)\n",
    "            current_members_str = \"; \".join(current_members) if current_members else None\n",
    "            inactive_members_str = \"; \".join(inactive_members) if inactive_members else None\n",
    "            former_members_str = \"; \".join(former_members) if former_members else None\n",
    "            \n",
    "            # SNS 링크를 문자열로 변환 (null 처리)\n",
    "            sns_str = json.dumps(group_info['sns_links'], ensure_ascii=False) if group_info['sns_links'] else None\n",
    "            \n",
    "            row_data = {\n",
    "                'group_name_en': group_info['group_name_en'],\n",
    "                'group_name_kr': group_info['group_name_hangul'] or None,\n",
    "                'debut_date': group_info['debut_date'] or None,\n",
    "                'entertainment_name': group_info['entertainment'] or None,\n",
    "                'entertainment_link': group_info['entertainment_link'] or None,\n",
    "                'member_current': current_members_str,\n",
    "                'member_inactive': inactive_members_str,\n",
    "                'member_former': former_members_str,\n",
    "                'fandom_name': group_info['fandom_name'] or None,\n",
    "                'sns': sns_str,\n",
    "                'group_type': group_type,\n",
    "                'update_at': current_time\n",
    "            }\n",
    "            \n",
    "            all_data.append(row_data)\n",
    "        \n",
    "        # 데이터가 있든 없든 항상 DataFrame 생성\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"CSV 파일이 생성되었습니다: {output_filename}\")\n",
    "        print(f\"총 {len(member_pages)}명의 멤버 페이지명이 수집되었습니다.\")\n",
    "        \n",
    "        return df, member_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec93de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤러 객체 생성\n",
    "crawler = KpopGroupCrawler_memberDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd62518",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# girl 그룹 내보내기\u001b[39;00m\n\u001b[1;32m      2\u001b[0m girl_groups \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed_Velvet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLACKPINK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTWICE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mITZY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAespa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAMAMOO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrazAngel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mILLIT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBABYMONSTER\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNJZ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m df, girl_groups_member_pages \u001b[38;5;241m=\u001b[39m \u001b[43mcrawler\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_csv_from_groups(girl_groups, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgirl_groups_info.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGirl Group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crawler' is not defined"
     ]
    }
   ],
   "source": [
    "# girl 그룹 내보내기\n",
    "girl_groups = [\"Red_Velvet\", \"BLACKPINK\", \"TWICE\", \"ITZY\", \"Aespa\", \"MAMAMOO\", \"CrazAngel\", \"IVE\", \"ILLIT\", \"BABYMONSTER\", \"NJZ\"]\n",
    "\n",
    "df, girl_groups_member_pages = crawler.create_csv_from_groups(girl_groups, \"girl_groups_info.csv\", \"Girl Group\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링 중: BTS\n",
      "크롤링 중: Stray_Kids\n",
      "크롤링 중: SEVENTEEN\n",
      "크롤링 중: ENHYPEN\n",
      "크롤링 중: TXT\n",
      "크롤링 중: NCT\n",
      "크롤링 중: NCT_127\n",
      "크롤링 중: ATEEZ\n",
      "크롤링 중: EXO\n",
      "크롤링 중: P1Harmony\n",
      "크롤링 중: SF9\n",
      "크롤링 중: VICTON\n",
      "크롤링 중: CRAVITY\n",
      "CSV 파일이 생성되었습니다: girl_groups_info.csv\n",
      "총 111명의 멤버 페이지명이 수집되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# boy 그룹 내보내기\n",
    "boy_groups = [\"BTS\", \"Stray_Kids\", \"SEVENTEEN\", \"ENHYPEN\", \"TXT\", \"NCT\", \"NCT_127\", \"ATEEZ\", \"EXO\", \"P1Harmony\", \"SF9\", \"VICTON\", \"CRAVITY\"]\n",
    "# 유닛 포함해서 그룹 내 멤버 중복되게 내보내기\n",
    "\n",
    "df, boy_groups_member_pages = crawler.create_csv_from_groups(boy_groups, \"boy_groups_info.csv\", \"Boy Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3194742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Empty DataFrame\n",
      "Columns: [group_name, page_name, member_name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# NCT 멤버 중복되었는지 확인\n",
    "\n",
    "df_duplicated_check = pd.DataFrame(boy_groups_member_pages)\n",
    "print(df_duplicated_check.duplicated(subset=['page_name']).sum())  # 중복 개수\n",
    "print(df_duplicated_check[df_duplicated_check.duplicated(subset=['page_name'], keep=False)])  # 중복된 행 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acd4f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정값\n",
    "base_url = \"https://kpop.fandom.com/api.php\"\n",
    "base_wiki_url = \"https://kpop.fandom.com\"\n",
    "\n",
    "def get_birth_name(soup):\n",
    "    \"\"\"본명 추출\"\"\"\n",
    "    selectors = [\n",
    "        '[data-source=\"birth_name\"] .pi-data-value',\n",
    "        '[data-source=\"birthname\"] .pi-data-value',\n",
    "        '[data-source=\"real_name\"] .pi-data-value',\n",
    "        '[data-source=\"realname\"] .pi-data-value'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        birth_name = soup.select_one(selector)\n",
    "        if birth_name:\n",
    "            return birth_name.get_text(strip=True)\n",
    "    return \"\"\n",
    "\n",
    "def get_birth_date_artist(soup):\n",
    "    \"\"\"생년월일 추출\"\"\"\n",
    "    birth_block = soup.select_one('[data-source=\"birth_date\"] .pi-data-value')\n",
    "    if birth_block:\n",
    "        birth_text = birth_block.get_text(strip=True)\n",
    "        birth_date_str = birth_text.split(\"(\")[0].strip()\n",
    "        \n",
    "        try:\n",
    "            birth_date = datetime.strptime(birth_date_str, \"%B %d, %Y\")\n",
    "            return birth_date.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            return birth_date_str\n",
    "    return \"\"\n",
    "\n",
    "def _get_entertainment(soup) -> str:\n",
    "    \"\"\"소속사명 추출 - 첫 번째 엔터테인먼트 회사\"\"\"\n",
    "    label_block = soup.select_one('[data-source=\"agency\"] .pi-data-value')\n",
    "    if not label_block:\n",
    "        return \"\"\n",
    "        \n",
    "    # 첫 번째 b 태그(국가 라벨) 다음의 첫 번째 a 태그나 텍스트 찾기\n",
    "    first_b = label_block.find('b')\n",
    "    if first_b:\n",
    "        # b 태그 다음의 첫 번째 a 태그 찾기\n",
    "        a_tag = first_b.find_next('a')\n",
    "        if a_tag:\n",
    "            return a_tag.get_text(strip=True)\n",
    "        \n",
    "    # b 태그가 없으면 기존 방식\n",
    "    labels = label_block.get_text(\"||\", strip=True).split(\"||\")\n",
    "    return labels[0].strip() if labels else \"\"\n",
    "    \n",
    "def _get_entertainment_link(soup) -> str:\n",
    "    \"\"\"소속사 링크 추출 - 첫 번째 엔터테인먼트 회사 링크\"\"\"\n",
    "    label_block = soup.select_one('[data-source=\"agency\"] .pi-data-value')\n",
    "    if not label_block:\n",
    "        return \"\"\n",
    "        \n",
    "    # 첫 번째 b 태그(국가 라벨) 다음의 첫 번째 a 태그 찾기\n",
    "    first_b = label_block.find('b')\n",
    "    if first_b:\n",
    "        a_tag = first_b.find_next('a', href=True)\n",
    "        if a_tag:\n",
    "            href = a_tag['href']\n",
    "            return href if href.startswith('http') else f\"{base_wiki_url}{href}\"\n",
    "        \n",
    "    # b 태그가 없으면 기존 방식\n",
    "    labels = label_block.get_text(\"||\", strip=True).split(\"||\")\n",
    "    if labels:\n",
    "        first_label = labels[0].strip()\n",
    "        a_tag = label_block.find('a', string=lambda text: text and text.strip() == first_label)\n",
    "        if a_tag and a_tag.has_attr('href'):\n",
    "            href = a_tag['href']\n",
    "            return href if href.startswith('http') else f\"{base_wiki_url}{href}\"\n",
    "        \n",
    "    return \"\"\n",
    "#\n",
    "\n",
    "def create_artist_csv(member_pages, output_filename=\"artists_info.csv\"):\n",
    "    \"\"\"\n",
    "    멤버 페이지 리스트를 받아서 개별 아티스트 정보를 크롤링하여 CSV로 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        member_pages (List[Dict]): [{'group_name': '그룹명', 'page_name': '페이지명'}, ...] 형태의 리스트\n",
    "        output_filename (str): 출력할 CSV 파일명\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 아티스트 정보가 담긴 DataFrame\n",
    "    \"\"\"\n",
    "    all_artist_data = []\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    for member_info in member_pages:\n",
    "        group_name = member_info['group_name']\n",
    "        page_name = member_info['page_name']\n",
    "        member_name = member_info['member_name']\n",
    "        \n",
    "        print(f\"크롤링 중: {group_name} - {page_name}\")\n",
    "        \n",
    "        try:\n",
    "            params = {\n",
    "                \"action\": \"parse\",\n",
    "                \"page\": page_name,\n",
    "                \"format\": \"json\"\n",
    "            }\n",
    "            \n",
    "            res = requests.get(base_url, params=params)\n",
    "            res.raise_for_status()\n",
    "            data = res.json()\n",
    "            \n",
    "            if 'parse' not in data:\n",
    "                try:\n",
    "                    # API 호출용으로 디코딩\n",
    "                    decoded_page_name = unquote(page_name)\n",
    "                    \n",
    "                    params = {\n",
    "                        \"action\": \"parse\",\n",
    "                        \"page\": decoded_page_name,\n",
    "                        \"format\": \"json\"\n",
    "                    }\n",
    "                    res = requests.get(base_url, params=params)\n",
    "                    res.raise_for_status()\n",
    "                    data = res.json()\n",
    "                except Exception as e:\n",
    "                    print(f\"페이지를 찾을 수 없습니다: {page_name}\")\n",
    "                    row_data = {\n",
    "                        'group_name': group_name,\n",
    "                        'page_name': page_name,\n",
    "                        'member_name': member_name,\n",
    "                        'birth_name': None,\n",
    "                        'birth_date': None,\n",
    "                        'agency_name': None,\n",
    "                        'agency_href': None,\n",
    "                        'update_at': current_time\n",
    "                    }\n",
    "                    all_artist_data.append(row_data)\n",
    "                    continue\n",
    "            \n",
    "            html = data[\"parse\"][\"text\"][\"*\"]\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # 정보 추출\n",
    "            birth_name = get_birth_name(soup)\n",
    "            birth_date = get_birth_date_artist(soup)\n",
    "            agency_name = _get_entertainment(soup)\n",
    "            agency_href = _get_entertainment_link(soup)\n",
    "            \n",
    "            row_data = {\n",
    "                'group_name': group_name,\n",
    "                'page_name': page_name,\n",
    "                'member_name': member_name,\n",
    "                'birth_name': birth_name or None,\n",
    "                'birth_date': birth_date or None,\n",
    "                'agency_name': agency_name or None,\n",
    "                'agency_href': agency_href or None,\n",
    "                'update_at': current_time\n",
    "            }\n",
    "            \n",
    "            all_artist_data.append(row_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"오류 ({page_name}): {e}\")\n",
    "            row_data = {\n",
    "                'group_name': group_name,\n",
    "                'page_name': page_name,\n",
    "                'member_name': member_name,\n",
    "                'birth_name': None,\n",
    "                'birth_date': None,\n",
    "                'agency_name': None,\n",
    "                'agency_href': None,\n",
    "                'update_at': current_time\n",
    "            }\n",
    "            all_artist_data.append(row_data)\n",
    "    \n",
    "    # CSV 저장\n",
    "    df = pd.DataFrame(all_artist_data)\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Artist CSV 파일이 생성되었습니다: {output_filename}\")\n",
    "    print(f\"총 {len(all_artist_data)}명의 아티스트 정보가 수집되었습니다.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "552c1537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링 중: BTS - Jin_(BTS)\n",
      "크롤링 중: BTS - Suga\n",
      "크롤링 중: BTS - J-Hope\n",
      "크롤링 중: BTS - RM\n",
      "크롤링 중: BTS - Jimin_(BTS)\n",
      "크롤링 중: BTS - V_(BTS)\n",
      "크롤링 중: BTS - Jung_Kook\n",
      "크롤링 중: Stray Kids - Bang_Chan\n",
      "크롤링 중: Stray Kids - Lee_Know\n",
      "크롤링 중: Stray Kids - Changbin_(Stray_Kids)\n",
      "크롤링 중: Stray Kids - Hyunjin_(Stray_Kids)\n",
      "크롤링 중: Stray Kids - Han_(Stray_Kids)\n",
      "크롤링 중: Stray Kids - Felix\n",
      "크롤링 중: Stray Kids - Seungmin_(Stray_Kids)\n",
      "크롤링 중: Stray Kids - I.N_(Stray_Kids)\n",
      "크롤링 중: SEVENTEEN - S.Coups\n",
      "크롤링 중: SEVENTEEN - Joshua\n",
      "크롤링 중: SEVENTEEN - Jun_(SEVENTEEN)\n",
      "크롤링 중: SEVENTEEN - DK_(SEVENTEEN)\n",
      "크롤링 중: SEVENTEEN - Mingyu_(SEVENTEEN)\n",
      "크롤링 중: SEVENTEEN - The8\n",
      "크롤링 중: SEVENTEEN - Seungkwan\n",
      "크롤링 중: SEVENTEEN - Vernon\n",
      "크롤링 중: SEVENTEEN - Dino_(SEVENTEEN)\n",
      "크롤링 중: SEVENTEEN - Jeonghan\n",
      "크롤링 중: SEVENTEEN - Hoshi\n",
      "크롤링 중: SEVENTEEN - Wonwoo_(SEVENTEEN)\n",
      "크롤링 중: SEVENTEEN - Woozi\n",
      "크롤링 중: ENHYPEN - Heeseung\n",
      "크롤링 중: ENHYPEN - Jay_(ENHYPEN)\n",
      "크롤링 중: ENHYPEN - Jake_(ENHYPEN)\n",
      "크롤링 중: ENHYPEN - Sunghoon_(ENHYPEN)\n",
      "크롤링 중: ENHYPEN - Sunoo\n",
      "크롤링 중: ENHYPEN - Jungwon_(ENHYPEN)\n",
      "크롤링 중: ENHYPEN - NI-KI\n",
      "크롤링 중: TXT - Yeonjun_(TXT)\n",
      "크롤링 중: TXT - Soobin_(TXT)\n",
      "크롤링 중: TXT - Beomgyu\n",
      "크롤링 중: TXT - Taehyun_(TXT)\n",
      "크롤링 중: TXT - Hueningkai\n",
      "크롤링 중: NCT - Johnny_(NCT)\n",
      "크롤링 중: NCT - Yuta\n",
      "크롤링 중: NCT - Kun_(NCT)\n",
      "크롤링 중: NCT - Doyoung_(NCT)\n",
      "크롤링 중: NCT - Ten_(NCT)\n",
      "크롤링 중: NCT - Jungwoo_(NCT)\n",
      "크롤링 중: NCT - Mark_(NCT)\n",
      "크롤링 중: NCT - Xiao_Jun\n",
      "크롤링 중: NCT - Hendery\n",
      "크롤링 중: NCT - Renjun\n",
      "크롤링 중: NCT - Jeno_(NCT)\n",
      "크롤링 중: NCT - Haechan\n",
      "크롤링 중: NCT - Jaemin_(NCT)\n",
      "크롤링 중: NCT - Yangyang\n",
      "크롤링 중: NCT - Chenle\n",
      "크롤링 중: NCT - Jisung_(NCT)\n",
      "크롤링 중: NCT - Sion_(NCT)\n",
      "크롤링 중: NCT - Riku_(NCT)\n",
      "크롤링 중: NCT - Yushi\n",
      "크롤링 중: NCT - Jaehee_(NCT)\n",
      "크롤링 중: NCT - Ryo_(NCT)\n",
      "크롤링 중: NCT - Sakuya\n",
      "크롤링 중: NCT - Taeyong_(NCT)\n",
      "크롤링 중: NCT - Jaehyun_(NCT)\n",
      "크롤링 중: NCT - Winwin\n",
      "크롤링 중: ATEEZ - Seonghwa\n",
      "크롤링 중: ATEEZ - Hongjoong\n",
      "크롤링 중: ATEEZ - Yunho_(ATEEZ)\n",
      "크롤링 중: ATEEZ - Yeosang\n",
      "크롤링 중: ATEEZ - San_(ATEEZ)\n",
      "크롤링 중: ATEEZ - Mingi\n",
      "크롤링 중: ATEEZ - Wooyoung_(ATEEZ)\n",
      "크롤링 중: ATEEZ - Jongho\n",
      "크롤링 중: EXO - Xiumin\n",
      "크롤링 중: EXO - Suho\n",
      "크롤링 중: EXO - Baekhyun\n",
      "크롤링 중: EXO - Chen_(EXO)\n",
      "크롤링 중: EXO - Chanyeol\n",
      "크롤링 중: EXO - D.O.\n",
      "크롤링 중: EXO - Kai_(EXO)\n",
      "크롤링 중: EXO - Sehun\n",
      "크롤링 중: EXO - Lay\n",
      "크롤링 중: P1Harmony - Theo_(P1Harmony)\n",
      "크롤링 중: P1Harmony - Keeho\n",
      "크롤링 중: P1Harmony - Jiung\n",
      "크롤링 중: P1Harmony - Intak\n",
      "크롤링 중: P1Harmony - Soul_(P1Harmony)\n",
      "크롤링 중: P1Harmony - Jongseob\n",
      "크롤링 중: SF9 - In_Seong_(SF9)\n",
      "크롤링 중: SF9 - Young_Bin_(SF9)\n",
      "크롤링 중: SF9 - Jae_Yoon_(SF9)\n",
      "크롤링 중: SF9 - Zu_Ho_(SF9)\n",
      "크롤링 중: SF9 - Yoo_Tae_Yang\n",
      "크롤링 중: SF9 - Hwi_Young\n",
      "크롤링 중: SF9 - Cha_Ni\n",
      "크롤링 중: SF9 - Da_Won_(SF9)\n",
      "크롤링 중: VICTON - Han_Seung_Woo\n",
      "크롤링 중: VICTON - Kang_Seung_Sik\n",
      "크롤링 중: VICTON - Lim_Se_Jun\n",
      "크롤링 중: VICTON - Do_Han_Se\n",
      "크롤링 중: VICTON - Choi_Byung_Chan\n",
      "크롤링 중: VICTON - Jung_Su_Bin\n",
      "크롤링 중: CRAVITY - Serim_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Allen_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Jungmo_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Woobin_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Wonjin_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Minhee_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Hyeongjun_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Taeyoung_(CRAVITY)\n",
      "크롤링 중: CRAVITY - Seongmin_(CRAVITY)\n",
      "Artist CSV 파일이 생성되었습니다: boy_groups_artists.csv\n",
      "총 111명의 아티스트 정보가 수집되었습니다.\n"
     ]
    }
   ],
   "source": [
    "artist_df = create_artist_csv(boy_groups_member_pages, \"boy_groups_artists.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b634d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
